{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, model_selection, metrics\n",
    "from sklearn.naive_bayes import CategoricalNB, GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('adult.csv')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are we missing in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.where(df == \"?\")).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove all rows that have any missing data, since it makes up only a negligible amount of the total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove messy rows\n",
    "print('total rows:', len(df))\n",
    "df_clean = df[df != '?'].dropna()\n",
    "print('rows removed:', len(df) - len(df_clean))\n",
    "df_clean\n",
    "(df_clean.where(df_clean == \"?\")).count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will make our training and test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names = list(df_clean.columns)\n",
    "var_names.remove('income')\n",
    "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(df_clean[var_names], df_clean['income'], test_size=0.33, random_state=42)\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(Y_train))\n",
    "print(len(Y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our preprocessing, we will label encode all of our categorical attributes, and do a standard scaling for all of our continuous attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess categorical data\n",
    "cat_names = ['workclass', 'education', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']\n",
    "df_cat_str = df_clean[cat_names]\n",
    "\n",
    "cat_encoders = {}\n",
    "\n",
    "X_train_cat = pd.DataFrame()\n",
    "X_test_cat = pd.DataFrame()\n",
    "for col in cat_names:\n",
    "    # fit encoders\n",
    "    encoder = preprocessing.LabelEncoder().fit(df_cat_str[col])\n",
    "    cat_encoders[col] = encoder\n",
    "\n",
    "    # transform data to labels\n",
    "    X_train_cat[col] = encoder.transform(X_train[col])\n",
    "    X_test_cat[col] = encoder.transform(X_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_names = ['age', 'fnlwgt', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week']\n",
    "df_cont = df_clean[cont_names]\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(df_cont)\n",
    "\n",
    "X_train_cont = pd.DataFrame()\n",
    "X_test_cont = pd.DataFrame()\n",
    "X_train_cont[cont_names] = scaler.transform(X_train[cont_names])\n",
    "X_test_cont[cont_names] = scaler.transform(X_test[cont_names])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we train our models over the categorical and continuous data seperately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_NB = CategoricalNB()\n",
    "cat_NB.fit(X_train_cat, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_NB = GaussianNB()\n",
    "cont_NB.fit(X_train_cont, Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict with both of our models, we can multiply the inferred probabilites from both of our models and choose the one with the greatest likelihood. It is actually implemented using the log probabilities, which are all negative, so instead we find the minimum of their multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df):\n",
    "    df = df.copy()\n",
    "    for col, encoder in cat_encoders.items():\n",
    "        df[col] = encoder.transform(df[col])\n",
    "    df[cont_names] = scaler.transform(df[cont_names])\n",
    "\n",
    "    # log probabilities to avoid underflow\n",
    "    cat_ps = cat_NB.predict_log_proba(df[cat_names])\n",
    "    cont_ps = cont_NB.predict_log_proba(df[cont_names])\n",
    "\n",
    "    combined_ps = cat_ps * cont_ps\n",
    "    return cont_NB.classes_[combined_ps.argmin(axis=1)]\n",
    "predict(df_clean)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is our accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = predict(X_test)\n",
    "metrics.accuracy_score(Y_hat, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
